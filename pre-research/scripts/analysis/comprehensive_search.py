#!/usr/bin/env python3
"""
CivitAI ç·å½“ãŸã‚Šæ¤œç´¢ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æŒ‡å®šã•ã‚ŒãŸBase Modelsã€Typesã€Tagsã€Sort Ordersã®å…¨çµ„ã¿åˆã‚ã›ã§æ¤œç´¢ã‚’å®Ÿè¡Œã—ã€
çµæœã‚’åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¿å­˜ã™ã‚‹ã€‚

Usage:
    python comprehensive_search.py --base-models illustrious noobai pony animagine --limit 500
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set
from dataclasses import dataclass, asdict

# Add src to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.api.client import CivitaiClient
from src.core.enhanced_url_collector import EnhancedURLCollector


@dataclass
class SearchResult:
    """æ¤œç´¢çµæœã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    base_model: str
    model_type: str
    tag: str
    sort_order: str
    found_count: int
    unique_ids: List[int]
    search_time: float
    api_calls: int
    timestamp: str
    error: str = None


class ComprehensiveSearcher:
    """ç·å½“ãŸã‚Šæ¤œç´¢ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, api_key: str, output_dir: str = "outputs/search_analysis"):
        self.client = CivitaiClient(api_key)
        self.enhanced_collector = EnhancedURLCollector(api_key=api_key)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Enhanced output directory
        self.enhanced_output_dir = Path(output_dir) / "enhanced"
        self.enhanced_output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ¤œç´¢ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.base_models = ["Illustrious", "NoobAI", "Pony", "Animagine"]
        self.model_types = ["Checkpoint", "LORA", "LyCORIS"]
        self.tags = ["style", "concept", "pose", "nsfw", "sexy", "hentai"]
        # åˆå›ã¯3ã¤ã®Sort Orderã«çµã‚‹
        self.sort_orders = [
            "Highest Rated", "Most Downloaded", "Most Liked"
        ]
        
        # 2å›ç›®ã®èª¿æŸ»ã§è¿½åŠ äºˆå®š
        self.sort_orders_phase2 = [
            "Most Images", "Most Collected", "Newest", "Oldest"
        ]
        
        # çµ±è¨ˆæƒ…å ±ï¼ˆModel Typeåˆ¥ã«åˆ†é›¢ï¼‰
        self.all_model_ids: Set[int] = set()
        self.model_ids_by_type: Dict[str, Set[int]] = {
            "Checkpoint": set(),
            "LORA": set(),
            "LyCORIS": set()
        }
        self.search_results: List[SearchResult] = []
        self.total_api_calls = 0
        self.start_time = datetime.now()
    
    def calculate_total_combinations(self) -> int:
        """ç·æ¤œç´¢çµ„ã¿åˆã‚ã›æ•°ã‚’è¨ˆç®—"""
        return len(self.base_models) * len(self.model_types) * len(self.tags) * len(self.sort_orders)
    
    def search_single_combination(self, 
                                base_model: str, 
                                model_type: str, 
                                tag: str, 
                                sort_order: str,
                                limit: int = 500) -> SearchResult:
        """å˜ä¸€ã®çµ„ã¿åˆã‚ã›ã§æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        
        search_start = time.time()
        api_calls = 0
        unique_ids = []
        models_data = []  # è©³ç´°ãªãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        error = None
        
        try:
            print(f"ğŸ” {base_model} + {model_type} + {tag} + {sort_order}")
            
            # APIæ¤œç´¢å®Ÿè¡Œ
            models = self.client.search_models_with_cursor(
                types=[model_type],
                tag=tag,
                base_models=[base_model],
                sort=sort_order,
                limit=100,  # APIåˆ¶é™
                max_pages=min(5, limit // 100 + 1)  # æœ€å¤§5ãƒšãƒ¼ã‚¸ã¾ãŸã¯æŒ‡å®šlimitåˆ†
            )
            
            # ãƒ¢ãƒ‡ãƒ«IDã‚’æŠ½å‡ºï¼ˆTypeåˆ¥ã«ç®¡ç†ï¼‰
            for model in models[:limit]:  # ä¸Šé™ã‚’é©ç”¨
                model_id = model.get('id')
                if model_id:
                    unique_ids.append(model_id)
                    models_data.append(model)  # è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
                    self.all_model_ids.add(model_id)
                    # Model Typeåˆ¥ã«ã‚‚è¿½åŠ 
                    if model_type in self.model_ids_by_type:
                        self.model_ids_by_type[model_type].add(model_id)
            
            # Enhanced output ã®ç”Ÿæˆï¼ˆå°‘é‡ã®å ´åˆã®ã¿ï¼‰
            if len(models_data) <= 100:  # å¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                self._save_enhanced_results(models_data, base_model, model_type, tag, sort_order)
            
            api_calls = min(5, limit // 100 + 1)  # å®Ÿéš›ã®APIå‘¼ã³å‡ºã—æ•°
            self.total_api_calls += api_calls
            
            print(f"  âœ… {len(unique_ids)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹")
            
        except Exception as e:
            error = str(e)
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {error}")
        
        search_time = time.time() - search_start
        
        return SearchResult(
            base_model=base_model,
            model_type=model_type,
            tag=tag,
            sort_order=sort_order,
            found_count=len(unique_ids),
            unique_ids=unique_ids,
            search_time=search_time,
            api_calls=api_calls,
            timestamp=datetime.now().isoformat(),
            error=error
        )
    
    def run_comprehensive_search(self, 
                               base_models: List[str] = None,
                               limit: int = 500,
                               save_frequency: int = 50) -> None:
        """ç·å½“ãŸã‚Šæ¤œç´¢ã‚’å®Ÿè¡Œ"""
        
        if base_models is None:
            base_models = self.base_models
        
        # æ¤œç´¢å¯¾è±¡ã®Base Modelsã‚’æ­£è¦åŒ–
        normalized_base_models = []
        for bm in base_models:
            if bm.lower() == "illustrious":
                normalized_base_models.append("Illustrious")
            elif bm.lower() == "noobai":
                normalized_base_models.append("NoobAI")
            elif bm.lower() == "pony":
                normalized_base_models.append("Pony")
            elif bm.lower() == "animagine":
                normalized_base_models.append("Animagine")
            else:
                normalized_base_models.append(bm.title())
        
        total_combinations = len(normalized_base_models) * len(self.model_types) * len(self.tags) * len(self.sort_orders)
        current_combination = 0
        
        print(f"ğŸš€ ç·å½“ãŸã‚Šæ¤œç´¢é–‹å§‹")
        print(f"ğŸ“Š æ¤œç´¢çµ„ã¿åˆã‚ã›æ•°: {total_combinations}")
        print(f"ğŸ“‹ Base Models: {normalized_base_models}")
        print(f"ğŸ¯ å„çµ„ã¿åˆã‚ã›ã®ä¸Šé™: {limit}ãƒ¢ãƒ‡ãƒ«")
        print(f"â±ï¸ æ¨å®šå®Ÿè¡Œæ™‚é–“: {total_combinations * 2 / 60:.1f}åˆ†")
        print()
        
        for base_model in normalized_base_models:
            print(f"\nğŸ” Base Model: {base_model} ã®æ¤œç´¢é–‹å§‹")
            base_model_results = []
            
            for model_type in self.model_types:
                for tag in self.tags:
                    for sort_order in self.sort_orders:
                        current_combination += 1
                        
                        print(f"ğŸ“ˆ é€²è¡ŒçŠ¶æ³: {current_combination}/{total_combinations} ({current_combination/total_combinations*100:.1f}%)")
                        
                        # æ¤œç´¢å®Ÿè¡Œ
                        result = self.search_single_combination(
                            base_model, model_type, tag, sort_order, limit
                        )
                        
                        self.search_results.append(result)
                        base_model_results.append(result)
                        
                        # å®šæœŸçš„ã«çµæœã‚’ä¿å­˜
                        if current_combination % save_frequency == 0:
                            self._save_intermediate_results(base_model)
                        
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                        time.sleep(0.5)
            
            # Base Modelæ¯ã«çµæœã‚’ä¿å­˜
            self._save_base_model_results(base_model, base_model_results)
            print(f"âœ… {base_model} ã®æ¤œç´¢å®Œäº† ({len(base_model_results)}çµ„ã¿åˆã‚ã›)")
        
        # æœ€çµ‚çµæœã‚’ä¿å­˜
        self._save_final_results()
        
        # å…¨ä½“ã® Enhanced å‡ºåŠ›ã‚’ç”Ÿæˆ
        self._generate_comprehensive_enhanced_output()
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self._print_summary()
    
    def _save_base_model_results(self, base_model: str, results: List[SearchResult]) -> None:
        """Base Modelæ¯ã®çµæœã‚’ä¿å­˜"""
        filename = f"{base_model.lower()}_search_results.json"
        filepath = self.output_dir / filename
        
        data = {
            "base_model": base_model,
            "total_combinations": len(results),
            "total_found_models": sum(r.found_count for r in results),
            "unique_model_ids": list(set(model_id for r in results for model_id in r.unique_ids)),
            "search_results": [asdict(result) for result in results],
            "timestamp": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ä¿å­˜å®Œäº†: {filepath}")
    
    def _save_intermediate_results(self, current_base_model: str) -> None:
        """ä¸­é–“çµæœã‚’ä¿å­˜ï¼ˆå†é–‹ç”¨ï¼‰"""
        filename = f"intermediate_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.output_dir / filename
        
        data = {
            "current_base_model": current_base_model,
            "total_combinations_completed": len(self.search_results),
            "unique_model_ids_so_far": list(self.all_model_ids),
            "search_results": [asdict(result) for result in self.search_results],
            "timestamp": datetime.now().isoformat()
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _save_final_results(self) -> None:
        """æœ€çµ‚çš„ãªçµ±åˆçµæœã‚’ä¿å­˜"""
        
        # 1. ç·åˆã‚µãƒãƒªãƒ¼
        summary_data = {
            "search_summary": {
                "total_combinations": len(self.search_results),
                "total_api_calls": self.total_api_calls,
                "total_found_models": sum(r.found_count for r in self.search_results),
                "unique_model_count": len(self.all_model_ids),
                "duplication_rate": 1 - (len(self.all_model_ids) / max(sum(r.found_count for r in self.search_results), 1)),
                "execution_time_minutes": (datetime.now() - self.start_time).total_seconds() / 60,
                "start_time": self.start_time.isoformat(),
                "end_time": datetime.now().isoformat()
            },
            "base_model_stats": self._calculate_base_model_stats(),
            "type_stats": self._calculate_type_stats(),
            "tag_stats": self._calculate_tag_stats(),
            "sort_stats": self._calculate_sort_stats()
        }
        
        summary_file = self.output_dir / "comprehensive_summary.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, ensure_ascii=False, indent=2)
        
        # 2. å…¨ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ID
        unique_models_file = self.output_dir / "final_unique_models.json"
        unique_data = {
            "unique_model_ids": list(self.all_model_ids),
            "count": len(self.all_model_ids),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(unique_models_file, 'w', encoding='utf-8') as f:
            json.dump(unique_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ æœ€çµ‚çµæœä¿å­˜å®Œäº†:")
        print(f"  ğŸ“Š ã‚µãƒãƒªãƒ¼: {summary_file}")
        print(f"  ğŸ†” ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«: {unique_models_file}")
    
    def _save_enhanced_results(self, models_data: List[Dict], base_model: str, model_type: str, tag: str, sort_order: str) -> None:
        """å€‹åˆ¥ã®æ¤œç´¢çµæœã‚’Enhancedå½¢å¼ã§ä¿å­˜"""
        if not models_data:
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        safe_filename = f"{base_model}_{model_type}_{tag}_{sort_order.replace(' ', '_')}_{timestamp}"
        
        try:
            # Enhanced URL Collectorã§æƒ…å ±ã‚’åé›†
            model_infos = self.enhanced_collector.collect_enhanced_model_info(models_data)
            
            if model_infos:
                # è¤‡æ•°å½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                exported_files = self.enhanced_collector.export_all_formats(
                    model_infos, 
                    safe_filename,
                    output_dir=self.enhanced_output_dir
                )
                
                print(f"  ğŸ’¾ Enhancedå‡ºåŠ›: {len(model_infos)}ãƒ¢ãƒ‡ãƒ« -> {safe_filename}")
        
        except Exception as e:
            print(f"  âš ï¸ Enhancedå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _generate_comprehensive_enhanced_output(self) -> None:
        """å…¨ä½“ã®æ¤œç´¢çµæœã‹ã‚‰ç·åˆEnhancedå‡ºåŠ›ã‚’ç”Ÿæˆ"""
        print(f"\nğŸ“¦ ç·åˆEnhancedå‡ºåŠ›ã‚’ç”Ÿæˆä¸­...")
        
        try:
            # Typeåˆ¥ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«IDã‚’åé›†
            for model_type, unique_ids in self.model_ids_by_type.items():
                if not unique_ids:
                    continue
                
                print(f"  ğŸ” {model_type}: {len(unique_ids)}ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’å‡¦ç†ä¸­...")
                
                # APIã§ãƒ¢ãƒ‡ãƒ«è©³ç´°ã‚’å–å¾—ï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
                models_data = []
                for i, model_id in enumerate(list(unique_ids)[:500]):  # æœ€å¤§5003067ã§åˆ¶é™
                    if i > 0 and i % 50 == 0:
                        print(f"    é€²è¡ŒçŠ¶æ³: {i}/{min(len(unique_ids), 500)}")
                        time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                    
                    try:
                        model_data = self.client.get_model_by_id(model_id)
                        if model_data:
                            models_data.append(model_data)
                    except Exception as e:
                        print(f"    âš ï¸ ãƒ¢ãƒ‡ãƒ«ID {model_id} ã®å–å¾—ã«å¤±æ•—: {e}")
                        continue
                
                if models_data:
                    # Enhancedå‡ºåŠ›ã‚’ç”Ÿæˆ
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"comprehensive_{model_type.lower()}_{timestamp}"
                    
                    model_infos = self.enhanced_collector.collect_enhanced_model_info(models_data)
                    if model_infos:
                        exported_files = self.enhanced_collector.export_all_formats(
                            model_infos, 
                            filename,
                            output_dir=self.enhanced_output_dir
                        )
                        
                        print(f"  âœ… {model_type} Enhancedå‡ºåŠ›å®Œäº†: {len(model_infos)}ãƒ¢ãƒ‡ãƒ«")
                        print(f"    CSV: {exported_files.get('csv', 'N/A')}")
                        print(f"    JSON: {exported_files.get('json', 'N/A')}")
                        print(f"    HTML: {exported_files.get('html', 'N/A')}")
        
        except Exception as e:
            print(f"  âŒ ç·åˆEnhancedå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _calculate_base_model_stats(self) -> Dict:
        """Base Modelåˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        stats = {}
        for base_model in self.base_models:
            results = [r for r in self.search_results if r.base_model == base_model]
            if results:
                unique_ids = set()
                for r in results:
                    unique_ids.update(r.unique_ids)
                
                stats[base_model] = {
                    "combinations": len(results),
                    "total_found": sum(r.found_count for r in results),
                    "unique_models": len(unique_ids),
                    "avg_models_per_search": sum(r.found_count for r in results) / len(results)
                }
        return stats
    
    def _calculate_type_stats(self) -> Dict:
        """Typeåˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        stats = {}
        for model_type in self.model_types:
            results = [r for r in self.search_results if r.model_type == model_type]
            if results:
                unique_ids = set()
                for r in results:
                    unique_ids.update(r.unique_ids)
                
                stats[model_type] = {
                    "combinations": len(results),
                    "total_found": sum(r.found_count for r in results),
                    "unique_models": len(unique_ids)
                }
        return stats
    
    def _calculate_tag_stats(self) -> Dict:
        """Tagåˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        stats = {}
        for tag in self.tags:
            results = [r for r in self.search_results if r.tag == tag]
            if results:
                unique_ids = set()
                for r in results:
                    unique_ids.update(r.unique_ids)
                
                stats[tag] = {
                    "combinations": len(results),
                    "total_found": sum(r.found_count for r in results),
                    "unique_models": len(unique_ids)
                }
        return stats
    
    def _calculate_sort_stats(self) -> Dict:
        """Sort Orderåˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—"""
        stats = {}
        for sort_order in self.sort_orders:
            results = [r for r in self.search_results if r.sort_order == sort_order]
            if results:
                stats[sort_order] = {
                    "combinations": len(results),
                    "total_found": sum(r.found_count for r in results),
                    "avg_models_per_search": sum(r.found_count for r in results) / len(results)
                }
        return stats
    
    def _print_summary(self) -> None:
        """å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        execution_time = (datetime.now() - self.start_time).total_seconds() / 60
        total_found = sum(r.found_count for r in self.search_results)
        
        print(f"\n" + "="*60)
        print(f"ğŸ‰ ç·å½“ãŸã‚Šæ¤œç´¢å®Œäº†!")
        print(f"="*60)
        print(f"â±ï¸ å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}åˆ†")
        print(f"ğŸ” ç·æ¤œç´¢çµ„ã¿åˆã‚ã›: {len(self.search_results)}")
        print(f"ğŸ“ ç·APIå‘¼ã³å‡ºã—: {self.total_api_calls}")
        print(f"ğŸ“Š ç™ºè¦‹ãƒ¢ãƒ‡ãƒ«ç·æ•°: {total_found:,}")
        print(f"ğŸ†” ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«æ•°: {len(self.all_model_ids):,}")
        print(f"ğŸ“ˆ é‡è¤‡ç‡: {(1 - len(self.all_model_ids) / max(total_found, 1)) * 100:.1f}%")
        print(f"ğŸ’¾ çµæœä¿å­˜å…ˆ: {self.output_dir}")
        print(f"="*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="CivitAI ç·å½“ãŸã‚Šæ¤œç´¢å®Ÿè¡Œ"
    )
    
    parser.add_argument(
        "--base-models",
        nargs="+",
        default=["illustrious", "noobai", "pony", "animagine"],
        choices=["illustrious", "noobai", "pony", "animagine"],
        help="æ¤œç´¢å¯¾è±¡ã®Base Models (default: all)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=500,
        help="å„çµ„ã¿åˆã‚ã›ã‚ãŸã‚Šã®æ¤œç´¢ä¸Šé™ (default: 500)"
    )
    parser.add_argument(
        "--output-dir",
        default="outputs/search_analysis",
        help="å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: outputs/search_analysis)"
    )
    parser.add_argument(
        "--save-frequency",
        type=int,
        default=50,
        help="ä¸­é–“ä¿å­˜ã®é »åº¦ (default: 50çµ„ã¿åˆã‚ã›æ¯)"
    )
    
    args = parser.parse_args()
    
    # APIã‚­ãƒ¼å–å¾—
    api_key = os.getenv("CIVITAI_API_KEY")
    if not api_key:
        print("âŒ ã‚¨ãƒ©ãƒ¼: CIVITAI_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦ãã ã•ã„")
        sys.exit(1)
    
    # æ¤œç´¢å®Ÿè¡Œ
    searcher = ComprehensiveSearcher(api_key, args.output_dir)
    
    try:
        searcher.run_comprehensive_search(
            base_models=args.base_models,
            limit=args.limit,
            save_frequency=args.save_frequency
        )
    except KeyboardInterrupt:
        print("\nâš ï¸ æ¤œç´¢ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        print("ğŸ’¾ ä¸­é–“çµæœã‚’ä¿å­˜ä¸­...")
        searcher._save_intermediate_results("interrupted")
        print("âœ… ä¸­é–“çµæœä¿å­˜å®Œäº†")


if __name__ == "__main__":
    main()