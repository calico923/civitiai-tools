#!/usr/bin/env python3
"""
CivitAI é‡è¤‡åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ç·å½“ãŸã‚Šæ¤œç´¢çµæœã‹ã‚‰é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã€æœ€çµ‚çš„ãªãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚

Usage:
    python duplicate_analyzer.py --input-dir outputs/search_analysis --output comprehensive_analysis.json
"""

import os
import json
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Tuple
from collections import defaultdict, Counter


class DuplicateAnalyzer:
    """é‡è¤‡åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, input_dir: str):
        self.input_dir = Path(input_dir)
        self.search_results = []
        self.model_id_to_combinations = defaultdict(list)
        self.combination_to_models = defaultdict(set)
        
    def load_search_results(self) -> None:
        """æ¤œç´¢çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        print("ğŸ“‚ æ¤œç´¢çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        result_files = list(self.input_dir.glob("*_search_results.json"))
        
        if not result_files:
            print("âŒ æ¤œç´¢çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return
        
        for result_file in result_files:
            print(f"  ğŸ“„ èª­ã¿è¾¼ã¿ä¸­: {result_file.name}")
            
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                for search_result in data.get('search_results', []):
                    self.search_results.append(search_result)
                    
                    # çµ„ã¿åˆã‚ã›æƒ…å ±ã‚’æ§‹ç¯‰
                    combination = (
                        search_result['base_model'],
                        search_result['model_type'],
                        search_result['tag'],
                        search_result['sort_order']
                    )
                    
                    for model_id in search_result['unique_ids']:
                        self.model_id_to_combinations[model_id].append(combination)
                        self.combination_to_models[combination].add(model_id)
        
        print(f"âœ… {len(self.search_results)}å€‹ã®æ¤œç´¢çµæœã‚’èª­ã¿è¾¼ã¿å®Œäº†")
    
    def analyze_duplicates(self) -> Dict:
        """é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        print("\nğŸ” é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æä¸­...")
        
        # åŸºæœ¬çµ±è¨ˆ
        all_model_ids = set(self.model_id_to_combinations.keys())
        total_found_instances = sum(len(search['unique_ids']) for search in self.search_results)
        
        # é‡è¤‡åº¦åˆ¥åˆ†æ
        duplication_levels = Counter()
        for model_id, combinations in self.model_id_to_combinations.items():
            duplication_levels[len(combinations)] += 1
        
        # æœ€ã‚‚é‡è¤‡ã®å¤šã„ãƒ¢ãƒ‡ãƒ«
        most_duplicated = sorted(
            self.model_id_to_combinations.items(),
            key=lambda x: len(x[1]),
            reverse=True
        )[:10]
        
        # Base Modelåˆ¥é‡è¤‡åˆ†æ
        base_model_overlaps = self._analyze_base_model_overlaps()
        
        # Tagåˆ¥é‡è¤‡åˆ†æ
        tag_overlaps = self._analyze_tag_overlaps()
        
        # Sort OrderåŠ¹æœåˆ†æ
        sort_effectiveness = self._analyze_sort_effectiveness()
        
        analysis_result = {
            "basic_stats": {
                "total_search_combinations": len(self.search_results),
                "total_found_instances": total_found_instances,
                "unique_model_count": len(all_model_ids),
                "overall_duplication_rate": 1 - (len(all_model_ids) / max(total_found_instances, 1)),
                "average_duplication_per_model": total_found_instances / len(all_model_ids) if all_model_ids else 0
            },
            "duplication_levels": dict(duplication_levels),
            "most_duplicated_models": [
                {
                    "model_id": model_id,
                    "duplication_count": len(combinations),
                    "found_in_combinations": len(combinations),
                    "combinations": [
                        f"{c[0]}+{c[1]}+{c[2]}+{c[3]}" for c in combinations[:5]
                    ]  # æœ€åˆã®5çµ„ã¿åˆã‚ã›ã®ã¿è¡¨ç¤º
                }
                for model_id, combinations in most_duplicated
            ],
            "base_model_analysis": base_model_overlaps,
            "tag_analysis": tag_overlaps,
            "sort_effectiveness": sort_effectiveness,
            "timestamp": datetime.now().isoformat()
        }
        
        return analysis_result
    
    def _analyze_base_model_overlaps(self) -> Dict:
        """Base Modelé–“ã®é‡è¤‡ã‚’åˆ†æ"""
        base_models = ["Illustrious", "NoobAI", "Pony", "Animagine"]
        overlaps = {}
        
        # å„Base Modelã§ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«IDã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰
        base_model_sets = {}
        for base_model in base_models:
            model_ids = set()
            for search in self.search_results:
                if search['base_model'] == base_model:
                    model_ids.update(search['unique_ids'])
            base_model_sets[base_model] = model_ids
        
        # é‡è¤‡åˆ†æ
        for i, base_model1 in enumerate(base_models):
            for base_model2 in base_models[i+1:]:
                set1 = base_model_sets[base_model1]
                set2 = base_model_sets[base_model2]
                
                intersection = set1 & set2
                union = set1 | set2
                
                overlaps[f"{base_model1}_vs_{base_model2}"] = {
                    "base_model1_unique": len(set1),
                    "base_model2_unique": len(set2),
                    "intersection": len(intersection),
                    "union": len(union),
                    "jaccard_similarity": len(intersection) / len(union) if union else 0,
                    "overlap_rate1": len(intersection) / len(set1) if set1 else 0,
                    "overlap_rate2": len(intersection) / len(set2) if set2 else 0
                }
        
        return overlaps
    
    def _analyze_tag_overlaps(self) -> Dict:
        """Tagé–“ã®é‡è¤‡ã‚’åˆ†æ"""
        tags = ["style", "concept", "pose", "nsfw", "sexy", "hentai"]
        tag_stats = {}
        
        for tag in tags:
            model_ids = set()
            search_count = 0
            
            for search in self.search_results:
                if search['tag'] == tag:
                    model_ids.update(search['unique_ids'])
                    search_count += 1
            
            tag_stats[tag] = {
                "unique_models": len(model_ids),
                "search_combinations": search_count,
                "avg_models_per_combination": len(model_ids) / search_count if search_count > 0 else 0
            }
        
        return tag_stats
    
    def _analyze_sort_effectiveness(self) -> Dict:
        """Sort Orderåˆ¥ã®åŠ¹æœã‚’åˆ†æ"""
        sort_orders = [
            "Highest Rated", "Most Downloaded", "Most Liked",
            "Most Images", "Most Collected", "Newest", "Oldest"
        ]
        
        sort_stats = {}
        
        for sort_order in sort_orders:
            model_ids = set()
            search_count = 0
            total_found = 0
            
            for search in self.search_results:
                if search['sort_order'] == sort_order:
                    model_ids.update(search['unique_ids'])
                    search_count += 1
                    total_found += search['found_count']
            
            sort_stats[sort_order] = {
                "unique_models": len(model_ids),
                "search_combinations": search_count,
                "total_found_instances": total_found,
                "avg_models_per_combination": total_found / search_count if search_count > 0 else 0,
                "unique_discovery_rate": len(model_ids) / total_found if total_found > 0 else 0
            }
        
        return sort_stats
    
    def generate_optimized_search_plan(self) -> Dict:
        """åŠ¹ç‡çš„ãªæ¤œç´¢è¨ˆç”»ã‚’ç”Ÿæˆ"""
        print("\nğŸ¯ æœ€é©åŒ–ã•ã‚ŒãŸæ¤œç´¢è¨ˆç”»ã‚’ç”Ÿæˆä¸­...")
        
        # å„çµ„ã¿åˆã‚ã›ã®åŠ¹ç‡æ€§ã‚’è©•ä¾¡
        combination_efficiency = []
        
        for search in self.search_results:
            if search['found_count'] > 0:
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ç™ºè¦‹ç‡ã‚’è¨ˆç®—
                unique_models_in_this_search = set(search['unique_ids'])
                
                # ã“ã®çµ„ã¿åˆã‚ã›ã§ã®ã¿ç™ºè¦‹ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ•°
                exclusive_models = 0
                for model_id in unique_models_in_this_search:
                    if len(self.model_id_to_combinations[model_id]) == 1:
                        exclusive_models += 1
                
                efficiency = {
                    "combination": f"{search['base_model']}+{search['model_type']}+{search['tag']}+{search['sort_order']}",
                    "found_count": search['found_count'],
                    "exclusive_models": exclusive_models,
                    "efficiency_score": exclusive_models / search['found_count'] if search['found_count'] > 0 else 0,
                    "search_time": search['search_time']
                }
                
                combination_efficiency.append(efficiency)
        
        # åŠ¹ç‡æ€§ã§ã‚½ãƒ¼ãƒˆ
        combination_efficiency.sort(key=lambda x: x['efficiency_score'], reverse=True)
        
        # ä¸Šä½çµ„ã¿åˆã‚ã›ã§å…¨ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ã‚’ã‚«ãƒãƒ¼ã§ãã‚‹ã‹åˆ†æ
        covered_models = set()
        optimal_combinations = []
        
        for combo in combination_efficiency:
            # ã“ã®çµ„ã¿åˆã‚ã›ã§æ–°ãŸã«ç™ºè¦‹ã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«æ•°ã‚’è¨ˆç®—
            combo_parts = combo['combination'].split('+')
            if len(combo_parts) == 4:
                base_model, model_type, tag, sort_order = combo_parts
                
                search_result = next(
                    (s for s in self.search_results 
                     if s['base_model'] == base_model and 
                        s['model_type'] == model_type and 
                        s['tag'] == tag and 
                        s['sort_order'] == sort_order),
                    None
                )
                
                if search_result:
                    new_models = set(search_result['unique_ids']) - covered_models
                    if new_models:
                        optimal_combinations.append({
                            "combination": combo['combination'],
                            "new_models_discovered": len(new_models),
                            "cumulative_coverage": len(covered_models) + len(new_models),
                            "efficiency_score": combo['efficiency_score']
                        })
                        covered_models.update(new_models)
                        
                        # 95%ã®ã‚«ãƒãƒ¼ç‡ã«é”ã—ãŸã‚‰çµ‚äº†
                        total_unique = len(self.model_id_to_combinations)
                        if len(covered_models) / total_unique >= 0.95:
                            break
        
        return {
            "total_combinations_analyzed": len(self.search_results),
            "total_unique_models": len(self.model_id_to_combinations),
            "optimal_combination_count": len(optimal_combinations),
            "coverage_with_optimal": len(covered_models),
            "coverage_percentage": len(covered_models) / len(self.model_id_to_combinations) * 100,
            "reduction_rate": (len(self.search_results) - len(optimal_combinations)) / len(self.search_results) * 100,
            "optimal_combinations": optimal_combinations[:20]  # ä¸Šä½20çµ„ã¿åˆã‚ã›
        }
    
    def export_final_results(self, output_file: str) -> None:
        """æœ€çµ‚åˆ†æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        print(f"\nğŸ“Š æœ€çµ‚åˆ†æçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­: {output_file}")
        
        # é‡è¤‡åˆ†æå®Ÿè¡Œ
        duplicate_analysis = self.analyze_duplicates()
        
        # æœ€é©åŒ–è¨ˆç”»ç”Ÿæˆ
        optimization_plan = self.generate_optimized_search_plan()
        
        # æœ€çµ‚ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆ
        final_unique_models = list(self.model_id_to_combinations.keys())
        
        final_results = {
            "analysis_summary": {
                "analysis_date": datetime.now().isoformat(),
                "input_directory": str(self.input_dir),
                "total_search_results": len(self.search_results),
                "final_unique_model_count": len(final_unique_models)
            },
            "duplicate_analysis": duplicate_analysis,
            "optimization_plan": optimization_plan,
            "final_unique_models": final_unique_models,
            "recommendations": self._generate_recommendations(duplicate_analysis, optimization_plan)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_file}")
        
        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        self._print_analysis_summary(duplicate_analysis, optimization_plan)
    
    def _generate_recommendations(self, duplicate_analysis: Dict, optimization_plan: Dict) -> List[str]:
        """åˆ†æçµæœã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # é‡è¤‡ç‡ã«åŸºã¥ãæ¨å¥¨
        duplication_rate = duplicate_analysis['basic_stats']['overall_duplication_rate']
        if duplication_rate > 0.7:
            recommendations.append("é‡è¤‡ç‡ãŒ70%ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ¤œç´¢æˆ¦ç•¥ã®æ¤œè¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        # Base Modelåˆ†æã«åŸºã¥ãæ¨å¥¨
        base_analysis = duplicate_analysis['base_model_analysis']
        high_overlap_pairs = [
            pair for pair, data in base_analysis.items()
            if data['jaccard_similarity'] > 0.5
        ]
        if high_overlap_pairs:
            recommendations.append(f"ä»¥ä¸‹ã®Base Modelçµ„ã¿åˆã‚ã›ã§é«˜ã„é‡è¤‡ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ: {', '.join(high_overlap_pairs)}")
        
        # æœ€é©åŒ–ã«åŸºã¥ãæ¨å¥¨
        reduction_rate = optimization_plan['reduction_rate']
        if reduction_rate > 50:
            recommendations.append(f"æ¤œç´¢çµ„ã¿åˆã‚ã›ã‚’{reduction_rate:.1f}%å‰Šæ¸›ã—ã¦ã‚‚95%ã®ã‚«ãƒãƒ¼ç‡ã‚’é”æˆå¯èƒ½ã§ã™ã€‚")
        
        return recommendations
    
    def _print_analysis_summary(self, duplicate_analysis: Dict, optimization_plan: Dict) -> None:
        """åˆ†æçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        basic_stats = duplicate_analysis['basic_stats']
        
        print(f"\n" + "="*60)
        print(f"ğŸ“Š é‡è¤‡åˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print(f"="*60)
        print(f"ğŸ” ç·æ¤œç´¢çµ„ã¿åˆã‚ã›: {basic_stats['total_search_combinations']:,}")
        print(f"ğŸ“Š ç™ºè¦‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç·æ•°: {basic_stats['total_found_instances']:,}")
        print(f"ğŸ†” ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¢ãƒ‡ãƒ«æ•°: {basic_stats['unique_model_count']:,}")
        print(f"ğŸ“ˆ å…¨ä½“é‡è¤‡ç‡: {basic_stats['overall_duplication_rate']*100:.1f}%")
        print(f"ğŸ¯ æœ€é©çµ„ã¿åˆã‚ã›æ•°: {optimization_plan['optimal_combination_count']}")
        print(f"ğŸ’¡ åŠ¹ç‡åŒ–å¯èƒ½ç‡: {optimization_plan['reduction_rate']:.1f}%")
        print(f"="*60)


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="CivitAI é‡è¤‡åˆ†æå®Ÿè¡Œ"
    )
    
    parser.add_argument(
        "--input-dir",
        default="outputs/search_analysis",
        help="æ¤œç´¢çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: outputs/search_analysis)"
    )
    parser.add_argument(
        "--output",
        default="comprehensive_analysis.json",
        help="å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å (default: comprehensive_analysis.json)"
    )
    
    args = parser.parse_args()
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = DuplicateAnalyzer(args.input_dir)
    
    try:
        analyzer.load_search_results()
        analyzer.export_final_results(args.output)
        
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        raise


if __name__ == "__main__":
    main()