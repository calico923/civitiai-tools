#!/usr/bin/env python3
"""
CivitAI APIã§å…¨ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã«ãŠã‘ã‚‹baseModelã®åˆ†å¸ƒã‚’åŒ…æ‹¬çš„ã«èª¿æŸ»

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:
1. å„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆCheckpointã€LORAã€LoConã€TextualInversionã€VAEç­‰ï¼‰ã§ã®baseModelã®åˆ†å¸ƒèª¿æŸ»
2. ä¸»è¦ãªbaseModelã‚«ãƒ†ã‚´ãƒªã®å‡ºç¾é »åº¦åˆ†æ
3. baseModelãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å€¤ã®æ­£è¦åŒ–
4. çµ±è¨ˆçµæœã‚’CSVã¨JSONã§å‡ºåŠ›
"""

import os
import sys
import json
import csv
import time
from collections import defaultdict, Counter
from datetime import datetime
from typing import Dict, List, Optional, Set
import re

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

from src.api.client import CivitaiClient


class BaseModelAnalyzer:
    """BaseModelåˆ†å¸ƒã®åŒ…æ‹¬çš„åˆ†æ"""
    
    def __init__(self, api_key: str):
        self.client = CivitaiClient(api_key)
        self.results = {}
        self.normalized_basemodels = {}
        self.all_raw_basemodels = set()
        
        # èª¿æŸ»å¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—
        self.model_types = [
            "Checkpoint",
            "LORA",
            "LoCon",
            "TextualInversion",
            "VAE",
            "Hypernetwork",
            "AestheticGradient",
            "Embedding",
            "Poses",
            "Wildcards",
            "Workflows",
            "Other"
        ]
        
        # BaseModelã®æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«
        self.normalization_rules = {
            # SDXLç³»
            r'sdxl.*1\.0': 'SDXL 1.0',
            r'sdxl.*0\.9': 'SDXL 0.9',
            r'sdxl.*base': 'SDXL 1.0',
            r'sdxl': 'SDXL 1.0',
            r'sd.*xl': 'SDXL 1.0',
            
            # SDç³»
            r'sd.*1\.5': 'SD 1.5',
            r'sd.*1\.4': 'SD 1.4',
            r'sd.*2\.1': 'SD 2.1',
            r'sd.*2\.0': 'SD 2.0',
            r'stable.*diffusion.*1\.5': 'SD 1.5',
            r'stable.*diffusion.*1\.4': 'SD 1.4',
            r'stable.*diffusion.*2\.1': 'SD 2.1',
            r'stable.*diffusion.*2\.0': 'SD 2.0',
            
            # Ponyç³»
            r'pony.*diffusion.*v6': 'Pony Diffusion V6 XL',
            r'pony.*v6': 'Pony Diffusion V6 XL',
            r'pony.*diffusion': 'Pony Diffusion V6 XL',
            r'pony': 'Pony Diffusion V6 XL',
            
            # Illustriousç³»
            r'illustrious.*xl': 'Illustrious XL',
            r'illustrious': 'Illustrious XL',
            
            # NoobAIç³»
            r'noobai.*xl': 'NoobAI XL',
            r'noobai': 'NoobAI XL',
            
            # Fluxç³»
            r'flux.*1.*dev': 'FLUX.1 [dev]',
            r'flux.*1.*schnell': 'FLUX.1 [schnell]',
            r'flux.*dev': 'FLUX.1 [dev]',
            r'flux.*schnell': 'FLUX.1 [schnell]',
            r'flux': 'FLUX.1 [dev]',
            
            # ãã®ä»–
            r'other': 'Other',
            r'none': 'None',
            r'unknown': 'Unknown',
            r'n/a': 'N/A',
            r'custom': 'Custom',
        }
    
    def normalize_basemodel(self, basemodel: str) -> str:
        """baseModelã®åå‰ã‚’æ­£è¦åŒ–"""
        if not basemodel:
            return "Unknown"
        
        # å°æ–‡å­—ã«å¤‰æ›ã—ã¦æ¯”è¼ƒ
        normalized = basemodel.lower().strip()
        
        # æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨
        for pattern, replacement in self.normalization_rules.items():
            if re.search(pattern, normalized, re.IGNORECASE):
                return replacement
        
        # ãƒãƒƒãƒã—ãªã„å ´åˆã¯å…ƒã®å€¤ã‚’è¿”ã™ï¼ˆæœ€åˆã®æ–‡å­—ã‚’å¤§æ–‡å­—ã«ï¼‰
        return basemodel.strip()
    
    def analyze_model_type(self, model_type: str, sample_size: int = 200) -> Dict:
        """ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®baseModelåˆ†å¸ƒã‚’åˆ†æ"""
        print(f"\n{'='*60}")
        print(f"åˆ†æä¸­: {model_type}")
        print(f"{'='*60}")
        
        # ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—
        try:
            models = self.client.search_models_with_cursor(
                types=[model_type],
                sort="Highest Rated",
                limit=100,
                max_pages=max(1, sample_size // 100)  # sample_sizeã«åŸºã¥ã„ã¦ãƒšãƒ¼ã‚¸æ•°ã‚’èª¿æ•´
            )
            
            if not models:
                print(f"âš ï¸  {model_type}ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                return {
                    "type": model_type,
                    "total_models": 0,
                    "basemodel_distribution": {},
                    "normalized_distribution": {},
                    "sample_models": []
                }
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã«åˆ¶é™
            if len(models) > sample_size:
                models = models[:sample_size]
                print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚’{sample_size}å€‹ã«åˆ¶é™ã—ã¾ã—ãŸ")
            
            print(f"åˆ†æå¯¾è±¡: {len(models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«")
            
            # baseModelã®åˆ†å¸ƒã‚’é›†è¨ˆ
            raw_basemodels = []
            normalized_basemodels = []
            sample_models = []
            
            for model in models:
                # ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰æœ€æ–°ã®baseModelã‚’å–å¾—
                model_versions = model.get("modelVersions", [])
                if model_versions:
                    latest_version = model_versions[0]  # æœ€åˆã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒæœ€æ–°
                    base_model = latest_version.get("baseModel", "Unknown")
                else:
                    base_model = "Unknown"
                
                raw_basemodels.append(base_model)
                normalized = self.normalize_basemodel(base_model)
                normalized_basemodels.append(normalized)
                
                # åˆ†æç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                sample_models.append({
                    "id": model.get("id"),
                    "name": model.get("name", "Unknown"),
                    "raw_basemodel": base_model,
                    "normalized_basemodel": normalized,
                    "tags": model.get("tags", []),
                    "stats": model.get("stats", {}),
                    "created_at": model.get("createdAt", ""),
                    "updated_at": model.get("updatedAt", "")
                })
                
                # å…¨ä½“ã®çµ±è¨ˆç”¨
                self.all_raw_basemodels.add(base_model)
            
            # åˆ†å¸ƒã‚’è¨ˆç®—
            raw_distribution = dict(Counter(raw_basemodels))
            normalized_distribution = dict(Counter(normalized_basemodels))
            
            # çµæœã‚’è¡¨ç¤º
            print(f"\n--- Raw BaseModelåˆ†å¸ƒ ---")
            for basemodel, count in sorted(raw_distribution.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(models)) * 100
                print(f"  {basemodel}: {count}å€‹ ({percentage:.1f}%)")
            
            print(f"\n--- Normalized BaseModelåˆ†å¸ƒ ---")
            for basemodel, count in sorted(normalized_distribution.items(), key=lambda x: x[1], reverse=True):
                percentage = (count / len(models)) * 100
                print(f"  {basemodel}: {count}å€‹ ({percentage:.1f}%)")
            
            return {
                "type": model_type,
                "total_models": len(models),
                "basemodel_distribution": raw_distribution,
                "normalized_distribution": normalized_distribution,
                "sample_models": sample_models
            }
            
        except Exception as e:
            print(f"âŒ {model_type}ã®åˆ†æã§ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "type": model_type,
                "total_models": 0,
                "basemodel_distribution": {},
                "normalized_distribution": {},
                "sample_models": [],
                "error": str(e)
            }
    
    def run_comprehensive_analysis(self, sample_size_per_type: int = 200) -> Dict:
        """å…¨ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®åŒ…æ‹¬çš„åˆ†æã‚’å®Ÿè¡Œ"""
        print("CivitAI BaseModelåˆ†å¸ƒã®åŒ…æ‹¬çš„èª¿æŸ»ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"å„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size_per_type}")
        
        start_time = time.time()
        
        # å„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ†æ
        for model_type in self.model_types:
            result = self.analyze_model_type(model_type, sample_size_per_type)
            self.results[model_type] = result
            
            # é€²æ—è¡¨ç¤º
            print(f"\nâœ… {model_type}ã®åˆ†æå®Œäº†")
            time.sleep(1)  # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œ
        
        # å…¨ä½“çµ±è¨ˆã‚’è¨ˆç®—
        total_models = sum(r["total_models"] for r in self.results.values())
        overall_normalized = defaultdict(int)
        overall_raw = defaultdict(int)
        
        for result in self.results.values():
            for basemodel, count in result["normalized_distribution"].items():
                overall_normalized[basemodel] += count
            for basemodel, count in result["basemodel_distribution"].items():
                overall_raw[basemodel] += count
        
        # çµæœã‚’ã¾ã¨ã‚
        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "total_models_analyzed": total_models,
            "model_types_analyzed": len(self.model_types),
            "sample_size_per_type": sample_size_per_type,
            "overall_normalized_distribution": dict(overall_normalized),
            "overall_raw_distribution": dict(overall_raw),
            "type_specific_results": self.results,
            "normalization_rules": self.normalization_rules,
            "unique_raw_basemodels": len(self.all_raw_basemodels),
            "analysis_duration_seconds": time.time() - start_time
        }
        
        return analysis_result
    
    def save_results(self, results: Dict, output_dir: str = "outputs/basemodel_analysis"):
        """çµæœã‚’CSVã¨JSONã§ä¿å­˜"""
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONå½¢å¼ã§ä¿å­˜
        json_file = os.path.join(output_dir, f"basemodel_analysis_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“„ JSONçµæœã‚’ä¿å­˜: {json_file}")
        
        # CSVå½¢å¼ã§ä¿å­˜ï¼ˆæ¦‚è¦ï¼‰
        csv_file = os.path.join(output_dir, f"basemodel_summary_{timestamp}.csv")
        with open(csv_file, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow([
                "Model Type",
                "Total Models", 
                "Top BaseModel",
                "Top BaseModel Count",
                "Top BaseModel Percentage",
                "Unique BaseModels"
            ])
            
            # å„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®æ¦‚è¦
            for model_type, result in results["type_specific_results"].items():
                if result["total_models"] > 0:
                    normalized_dist = result["normalized_distribution"]
                    if normalized_dist:
                        top_basemodel = max(normalized_dist.items(), key=lambda x: x[1])
                        top_name, top_count = top_basemodel
                        top_percentage = (top_count / result["total_models"]) * 100
                        unique_count = len(normalized_dist)
                    else:
                        top_name, top_count, top_percentage, unique_count = "None", 0, 0, 0
                    
                    writer.writerow([
                        model_type,
                        result["total_models"],
                        top_name,
                        top_count,
                        f"{top_percentage:.1f}%",
                        unique_count
                    ])
        
        print(f"ğŸ“Š CSVæ¦‚è¦ã‚’ä¿å­˜: {csv_file}")
        
        # è©³ç´°ãªBaseModelåˆ†å¸ƒCSV
        detail_csv = os.path.join(output_dir, f"basemodel_distribution_{timestamp}.csv")
        with open(detail_csv, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼
            writer.writerow([
                "Model Type",
                "BaseModel (Normalized)",
                "BaseModel (Raw)",
                "Count",
                "Percentage"
            ])
            
            # å„ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ã®è©³ç´°åˆ†å¸ƒ
            for model_type, result in results["type_specific_results"].items():
                if result["total_models"] > 0:
                    # æ­£è¦åŒ–ã•ã‚ŒãŸåˆ†å¸ƒã‚’å‡¦ç†
                    for norm_basemodel, count in sorted(result["normalized_distribution"].items(), key=lambda x: x[1], reverse=True):
                        percentage = (count / result["total_models"]) * 100
                        
                        # å¯¾å¿œã™ã‚‹rawãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
                        raw_basemodels = []
                        for model in result["sample_models"]:
                            if model["normalized_basemodel"] == norm_basemodel:
                                raw_basemodels.append(model["raw_basemodel"])
                        raw_unique = list(set(raw_basemodels))
                        
                        writer.writerow([
                            model_type,
                            norm_basemodel,
                            "; ".join(raw_unique),
                            count,
                            f"{percentage:.1f}%"
                        ])
        
        print(f"ğŸ“ˆ è©³ç´°CSVåˆ†å¸ƒã‚’ä¿å­˜: {detail_csv}")
        
        return {
            "json_file": json_file,
            "csv_summary": csv_file,
            "csv_detail": detail_csv
        }
    
    def print_summary(self, results: Dict):
        """åˆ†æçµæœã®æ¦‚è¦ã‚’è¡¨ç¤º"""
        print(f"\n{'='*80}")
        print("BaseModelåˆ†å¸ƒåˆ†æ - æ¦‚è¦")
        print(f"{'='*80}")
        
        print(f"ğŸ“Š åˆ†æå¯¾è±¡: {results['total_models_analyzed']:,}å€‹ã®ãƒ¢ãƒ‡ãƒ«")
        print(f"ğŸ” ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—æ•°: {results['model_types_analyzed']}ç¨®é¡")
        print(f"â±ï¸  åˆ†ææ™‚é–“: {results['analysis_duration_seconds']:.1f}ç§’")
        print(f"ğŸ”§ ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªraw BaseModel: {results['unique_raw_basemodels']}ç¨®é¡")
        
        print(f"\n--- å…¨ä½“ã®BaseModelåˆ†å¸ƒ (æ­£è¦åŒ–å¾Œ) ---")
        for basemodel, count in sorted(results["overall_normalized_distribution"].items(), key=lambda x: x[1], reverse=True)[:10]:
            percentage = (count / results["total_models_analyzed"]) * 100
            print(f"  {basemodel}: {count:,}å€‹ ({percentage:.1f}%)")
        
        print(f"\n--- ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥æ¦‚è¦ ---")
        for model_type, result in results["type_specific_results"].items():
            if result["total_models"] > 0:
                print(f"  {model_type}: {result['total_models']}å€‹")
                if result["normalized_distribution"]:
                    top_basemodel = max(result["normalized_distribution"].items(), key=lambda x: x[1])
                    top_name, top_count = top_basemodel
                    top_percentage = (top_count / result["total_models"]) * 100
                    print(f"    â†’ æœ€å¤š: {top_name} ({top_count}å€‹, {top_percentage:.1f}%)")
                else:
                    print(f"    â†’ BaseModelãƒ‡ãƒ¼ã‚¿ãªã—")
        
        print(f"\n{'='*80}")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    # APIã‚­ãƒ¼ã®ç¢ºèª
    api_key = os.getenv("CIVITAI_API_KEY")
    if not api_key:
        print("âŒ CIVITAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("export CIVITAI_API_KEY=your_api_key_here")
        sys.exit(1)
    
    # åˆ†æå®Ÿè¡Œ
    analyzer = BaseModelAnalyzer(api_key)
    
    try:
        # åŒ…æ‹¬çš„åˆ†æã‚’å®Ÿè¡Œ
        results = analyzer.run_comprehensive_analysis(sample_size_per_type=200)
        
        # çµæœã‚’ä¿å­˜
        saved_files = analyzer.save_results(results)
        
        # æ¦‚è¦ã‚’è¡¨ç¤º
        analyzer.print_summary(results)
        
        print(f"\nğŸ‰ åˆ†æå®Œäº†!")
        print(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
        for file_type, file_path in saved_files.items():
            print(f"  {file_type}: {file_path}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  åˆ†æãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()