# Civitai Model Downloader å®Ÿè£…è¨ˆç”»æ›¸

## 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦

### 1.1 ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå
Civitai Model Downloader

### 1.2 é–‹ç™ºæœŸé–“
ç´„2é€±é–“ï¼ˆå®Ÿåƒ10æ—¥ï¼‰

### 1.3 æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- **è¨€èª**: Python 3.8+
- **ä¸»è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**:
  - requests: HTTPé€šä¿¡
  - click: CLIæ§‹ç¯‰
  - rich: é€²æ—è¡¨ç¤ºãƒ»ã‚¿ãƒ¼ãƒŸãƒŠãƒ«UI
  - python-dotenv: ç’°å¢ƒå¤‰æ•°ç®¡ç†
  - pydantic: ãƒ‡ãƒ¼ã‚¿ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
  - sqlite3: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ç®¡ç†ï¼ˆæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰

## 2. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ

### 2.1 ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
```
civitiai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ client.py          # Civitai APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
â”‚   â”‚   â””â”€â”€ models.py          # APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ downloader.py      # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
â”‚   â”‚   â”œâ”€â”€ filter.py          # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
â”‚   â”‚   â”œâ”€â”€ cache.py           # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ history.py         # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ url_collector.py   # URLåé›†ãƒ»å‡ºåŠ›
â”‚   â”‚   â”œâ”€â”€ enhanced_url_collector.py  # æ‹¡å¼µURLåé›†ï¼ˆå€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å¯¾å¿œï¼‰
â”‚   â”‚   â””â”€â”€ web_scraper.py     # ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_handler.py    # ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œ
â”‚   â”‚   â”œâ”€â”€ logger.py          # ãƒ­ã‚®ãƒ³ã‚°
â”‚   â”‚   â””â”€â”€ progress.py        # é€²æ—è¡¨ç¤º
â”‚   â””â”€â”€ cli/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ main.py            # CLIã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ collection/
â”‚   â”‚   â”œâ”€â”€ enhanced_collection.py     # æ‹¡å¼µãƒ¢ãƒ‡ãƒ«åé›†
â”‚   â”‚   â”œâ”€â”€ url_model_collector.py     # å€‹åˆ¥URLåé›†
â”‚   â”‚   â””â”€â”€ batch_url_collector.py     # ä¸€æ‹¬URLåé›†
â”‚   â””â”€â”€ organize_outputs.py            # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ•´ç†
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py
â”‚   â”œâ”€â”€ test_downloader.py
â”‚   â””â”€â”€ test_filter.py
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ usage.md
â”‚   â””â”€â”€ api_setup.md
â”œâ”€â”€ models/                    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ loras/
â”œâ”€â”€ cache/                     # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ data/                      # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â””â”€â”€ history.db            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
â”œâ”€â”€ outputs/                   # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”œâ”€â”€ urls/                 # URLãƒªã‚¹ãƒˆå‡ºåŠ›
â”‚   â”œâ”€â”€ enhanced/             # æ‹¡å¼µãƒ¢ãƒ‡ãƒ«æƒ…å ±å‡ºåŠ›
â”‚   â”œâ”€â”€ reports/              # ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›
â”‚   â”œâ”€â”€ checkpoints/          # Checkpointæ•´ç†æ¸ˆã¿
â”‚   â”œâ”€â”€ loras/                # LoRAæ•´ç†æ¸ˆã¿
â”‚   â”œâ”€â”€ analysis/             # åˆ†æçµæœ
â”‚   â”œâ”€â”€ debug/                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
â”‚   â””â”€â”€ archive/              # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
â”œâ”€â”€ logs/                      # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â””â”€â”€ config.yaml               # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
```

### 2.2 ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ

#### 2.2.1 APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ (api/client.py)
```python
class CivitaiClient:
    - __init__(api_key: str)
    - search_models(params: dict) -> List[Model]
    - get_model_details(model_id: int) -> Model
    - get_model_by_id(model_id: int) -> Dict
    - get_model_from_url(civitai_url: str) -> Dict
    - search_models_with_cursor() -> List[Dict]
    - download_model(version_id: int) -> Response
```

#### 2.2.2 ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ (core/downloader.py)
```python
class ModelDownloader:
    - download_file(url: str, dest: Path) -> Path
    - batch_download(models: List[Model])
    - resume_download(partial_file: Path)
```

#### 2.2.3 ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (core/filter.py)
```python
class ModelFilter:
    - filter_lora_models(models: List[Model]) -> List[Model]
    - has_required_tags(model: Model) -> bool
    - remove_duplicates(models: List[Model]) -> List[Model]
```

#### 2.2.4 å±¥æ­´ç®¡ç† (core/history.py)
```python
class DownloadHistory:
    - __init__(db_path: Path)
    - add_download(model: Model, file_path: Path)
    - is_downloaded(model_id: int, version_id: int) -> bool
    - get_download_info(model_id: int) -> DownloadRecord
    - list_downloads(filter_params: dict) -> List[DownloadRecord]
    - cleanup_orphaned_records()
```

#### 2.2.5 URLåé›† (core/url_collector.py)
```python
class URLCollector:
    - __init__(output_dir: Path)
    - collect_model_urls(models: List[Model]) -> List[URLInfo]
    - export_to_text(urls: List[URLInfo], filename: str)
    - export_to_csv(urls: List[URLInfo], filename: str)
    - export_to_json(urls: List[URLInfo], filename: str)
```

#### 2.2.6 æ‹¡å¼µURLåé›† (core/enhanced_url_collector.py)
```python
class EnhancedURLCollector:
    - __init__(api_key: str, output_dir: Path)
    - collect_enhanced_model_info(models: List[Dict]) -> List[ModelInfo]
    - validate_download_urls(model_infos: List[ModelInfo]) -> List[ModelInfo]
    - export_all_formats(model_infos: List[ModelInfo], base_filename: str) -> Dict[str, Path]
    - export_html(model_infos: List[ModelInfo], filename: str) -> Path
```

#### 2.2.7 ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (core/web_scraper.py)
```python
class CivitaiWebScraper:
    - __init__(username: str, password: str)
    - login() -> bool
    - get_user_models(username: str) -> List[str]
    - get_restricted_model_urls(username: str) -> List[str]
    - extract_model_id_from_url(url: str) -> int
    - maintain_session() -> bool
```

## 3. å®Ÿè£…ãƒ•ã‚§ãƒ¼ã‚º

### Phase 1: åŸºç›¤æ§‹ç¯‰ï¼ˆ2æ—¥ï¼‰
- [x] ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã®ä½œæˆ
- [x] é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
- [x] åŸºæœ¬çš„ãªè¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
- [x] ãƒ­ã‚®ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…

### Phase 2: APIçµ±åˆï¼ˆ3æ—¥ï¼‰
- [x] Civitai APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å®Ÿè£…
- [x] èªè¨¼æ©Ÿèƒ½ã®å®Ÿè£…
- [x] ãƒ¢ãƒ‡ãƒ«æ¤œç´¢æ©Ÿèƒ½ã®å®Ÿè£…
- [x] ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯
- [x] å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«å–å¾—æ©Ÿèƒ½ï¼ˆURLæŒ‡å®šï¼‰
- [x] ã‚«ãƒ¼ã‚½ãƒ«ãƒ™ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ä¿®æ­£

### Phase 3: ã‚³ã‚¢æ©Ÿèƒ½å®Ÿè£…ï¼ˆ3æ—¥ï¼‰
- [x] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ©Ÿèƒ½
- [x] URLåé›†æ©Ÿèƒ½ï¼ˆURLCollectorï¼‰
- [x] æ‹¡å¼µURLåé›†æ©Ÿèƒ½ï¼ˆEnhancedURLCollectorï¼‰
- [x] å€‹åˆ¥ãƒ»ä¸€æ‹¬URLå–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- [x] å±¥æ­´ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆSQLiteï¼‰
- [ ] ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºä»˜ãï¼‰
- [ ] ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ 
- [ ] ä¸­æ–­ãƒ»å†é–‹æ©Ÿèƒ½

### Phase 4: åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«å–å¾—ï¼ˆæ–°è¦è¿½åŠ ï¼‰
- [ ] ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½å®Ÿè£…
- [ ] ãƒ­ã‚°ã‚¤ãƒ³èªè¨¼ã‚·ã‚¹ãƒ†ãƒ 
- [ ] ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†ã¨Cookieä¿æŒ
- [ ] åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«URLè‡ªå‹•åé›†
- [ ] ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹èªè¨¼æƒ…å ±ç®¡ç†

### Phase 5: CLIé–‹ç™ºï¼ˆ1æ—¥ï¼‰
- [x] ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼
- [ ] å¯¾è©±çš„ãƒ¢ãƒ¼ãƒ‰ã®å®Ÿè£…
- [x] ãƒãƒƒãƒå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
- [x] é€²æ—è¡¨ç¤ºUI

### Phase 6: ãƒ†ã‚¹ãƒˆã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆ1æ—¥ï¼‰
- [ ] ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã®ä½œæˆ
- [ ] çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿæ–½
- [x] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
- [x] ã‚µãƒ³ãƒ—ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ

## 4. è©³ç´°ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

### 4.1 Week 1
**Day 1-2: åŸºç›¤æ§‹ç¯‰**
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
- ä¾å­˜é–¢ä¿‚ã®è¨­å®š
- åŸºæœ¬ã‚¯ãƒ©ã‚¹æ§‹é€ ã®å®Ÿè£…
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æ©Ÿèƒ½

**Day 3-5: APIçµ±åˆ**
- APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ã®å®Ÿè£…
- æ¤œç´¢ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®çµ±åˆ
- ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å®Ÿè£…
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

### 4.2 Week 2
**Day 6-8: ã‚³ã‚¢æ©Ÿèƒ½**
- ã‚¿ã‚°ãƒ™ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ç®¡ç†
- ãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
- SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆãƒ»å®Ÿè£…
- å±¥æ­´ç®¡ç†æ©Ÿèƒ½ã®çµ±åˆ
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜

**Day 9: CLIé–‹ç™º**
- Click frameworkã§ã®CLIæ§‹ç¯‰
- Rich libraryã§ã®é€²æ—è¡¨ç¤º
- è¨­å®šã‚¦ã‚£ã‚¶ãƒ¼ãƒ‰

**Day 10: ä»•ä¸Šã’**
- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™
- ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒ³ã‚°

## 5. æŠ€è¡“çš„å®Ÿè£…è©³ç´°

### 5.1 APIé€šä¿¡
```python
# ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
class RateLimiter:
    def __init__(self, calls_per_second=0.5):
        self.min_interval = 1.0 / calls_per_second
        self.last_call = 0
    
    def wait_if_needed(self):
        elapsed = time.time() - self.last_call
        if elapsed < self.min_interval:
            time.sleep(self.min_interval - elapsed)
        self.last_call = time.time()
```

### 5.2 ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè£…
```python
def filter_lora_by_tags(model: dict) -> bool:
    """LoRAãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ãªã‚¿ã‚°æ¡ä»¶ã‚’æº€ãŸã™ã‹ãƒã‚§ãƒƒã‚¯"""
    required_base_tags = {'pony', 'illustrious', 'noobai'}
    required_type_tags = {'style', 'concept'}
    
    model_tags = {tag.lower() for tag in model.get('tags', [])}
    
    has_base_tag = bool(required_base_tags & model_tags)
    has_type_tag = bool(required_type_tags & model_tags)
    
    return has_base_tag and has_type_tag
```

### 5.3 ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å†é–‹æ©Ÿèƒ½
```python
def resume_download(url: str, partial_file: Path, headers: dict):
    """éƒ¨åˆ†çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†é–‹"""
    file_size = partial_file.stat().st_size
    headers['Range'] = f'bytes={file_size}-'
    
    response = requests.get(url, headers=headers, stream=True)
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰206ã¯éƒ¨åˆ†çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¤ºã™
    if response.status_code == 206:
        with open(partial_file, 'ab') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
```

### 5.4 ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ç®¡ç†ï¼ˆSQLiteå®Ÿè£…ï¼‰
```python
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict

class DownloadHistory:
    """SQLiteã‚’ä½¿ç”¨ã—ãŸãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ç®¡ç†"""
    
    def __init__(self, db_path: Path = Path("data/history.db")):
        self.db_path = db_path
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS download_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    model_id INTEGER NOT NULL,
                    version_id INTEGER NOT NULL,
                    model_name TEXT NOT NULL,
                    model_type TEXT NOT NULL,
                    file_path TEXT NOT NULL,
                    file_size INTEGER,
                    download_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    tags TEXT,  -- JSONå½¢å¼ã§ä¿å­˜
                    creator TEXT,
                    civitai_url TEXT,
                    status TEXT DEFAULT 'completed',
                    UNIQUE(model_id, version_id)
                )
            ''')
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ä½œæˆ
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_model_id 
                ON download_history(model_id)
            ''')
            cursor.execute('''
                CREATE INDEX IF NOT EXISTS idx_download_date 
                ON download_history(download_date)
            ''')
            conn.commit()
    
    def add_download(self, model: dict, file_path: Path) -> int:
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨˜éŒ²ã®è¿½åŠ """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO download_history 
                (model_id, version_id, model_name, model_type, 
                 file_path, file_size, tags, creator, civitai_url)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                model['id'],
                model['modelVersions'][0]['id'],
                model['name'],
                model['type'],
                str(file_path),
                file_path.stat().st_size if file_path.exists() else None,
                json.dumps(model.get('tags', [])),
                model['creator']['username'],
                f"https://civitai.com/models/{model['id']}"
            ))
            conn.commit()
            return cursor.lastrowid
    
    def is_downloaded(self, model_id: int, version_id: int) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ç¢ºèª"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute('''
                SELECT file_path FROM download_history
                WHERE model_id = ? AND version_id = ? AND status = 'completed'
            ''', (model_id, version_id))
            result = cursor.fetchone()
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å®Ÿä½“ã‚‚ç¢ºèª
            if result:
                file_path = Path(result[0])
                return file_path.exists()
            return False
    
    def get_download_info(self, model_id: int) -> Optional[Dict]:
        """ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æƒ…å ±ã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT * FROM download_history
                WHERE model_id = ?
                ORDER BY download_date DESC
                LIMIT 1
            ''', (model_id,))
            row = cursor.fetchone()
            return dict(row) if row else None
    
    def list_downloads(self, 
                      model_type: Optional[str] = None,
                      days_ago: Optional[int] = None) -> List[Dict]:
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã®ä¸€è¦§å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            query = "SELECT * FROM download_history WHERE 1=1"
            params = []
            
            if model_type:
                query += " AND model_type = ?"
                params.append(model_type)
            
            if days_ago:
                query += " AND download_date >= datetime('now', ?)"
                params.append(f'-{days_ago} days')
            
            query += " ORDER BY download_date DESC"
            
            cursor.execute(query, params)
            return [dict(row) for row in cursor.fetchall()]
    
    def cleanup_orphaned_records(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„è¨˜éŒ²ã‚’å‰Šé™¤"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT id, file_path FROM download_history")
            
            orphaned_ids = []
            for row in cursor.fetchall():
                if not Path(row[1]).exists():
                    orphaned_ids.append(row[0])
            
            if orphaned_ids:
                placeholders = ','.join('?' * len(orphaned_ids))
                cursor.execute(
                    f"DELETE FROM download_history WHERE id IN ({placeholders})",
                    orphaned_ids
                )
                conn.commit()
                print(f"å‰Šé™¤ã•ã‚ŒãŸå­¤ç«‹ãƒ¬ã‚³ãƒ¼ãƒ‰: {len(orphaned_ids)}ä»¶")

# ä½¿ç”¨ä¾‹
history = DownloadHistory()

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‰ã®ãƒã‚§ãƒƒã‚¯
if not history.is_downloaded(model_id=12345, version_id=67890):
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†
    file_path = download_model(model)
    history.add_download(model, file_path)

# å±¥æ­´ã®ç¢ºèª
recent_downloads = history.list_downloads(model_type="LORA", days_ago=7)
for download in recent_downloads:
    print(f"{download['model_name']} - {download['download_date']}")
```

### 5.5 å±¥æ­´ç®¡ç†ã®æ´»ç”¨æ–¹æ³•

#### å†ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é˜²æ­¢
```python
def should_download(model: dict, history: DownloadHistory) -> bool:
    """ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã¹ãã‹åˆ¤å®š"""
    version_id = model['modelVersions'][0]['id']
    
    # æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã‹ç¢ºèª
    if history.is_downloaded(model['id'], version_id):
        logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {model['name']} (æ—¢ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿)")
        return False
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®ãƒã‚§ãƒƒã‚¯
    if not check_disk_space(model):
        return False
    
    return True
```

#### CLIçµ±åˆä¾‹
```python
@click.command()
@click.option('--list-history', is_flag=True, help='ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´ã‚’è¡¨ç¤º')
@click.option('--cleanup', is_flag=True, help='å­¤ç«‹ã—ãŸå±¥æ­´ã‚’å‰Šé™¤')
def main(list_history, cleanup):
    history = DownloadHistory()
    
    if list_history:
        downloads = history.list_downloads()
        console = Console()
        table = Table(title="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å±¥æ­´")
        table.add_column("ãƒ¢ãƒ‡ãƒ«å", style="cyan")
        table.add_column("ã‚¿ã‚¤ãƒ—", style="magenta")
        table.add_column("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ—¥", style="green")
        table.add_column("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹", style="yellow")
        
        for dl in downloads:
            table.add_row(
                dl['model_name'],
                dl['model_type'],
                dl['download_date'],
                dl['file_path']
            )
        console.print(table)
    
    elif cleanup:
        history.cleanup_orphaned_records()
```

### 5.6 URLåé›†ã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰åˆ¶å¾¡

#### ç’°å¢ƒå¤‰æ•°ã«ã‚ˆã‚‹å‹•ä½œãƒ¢ãƒ¼ãƒ‰åˆ‡ã‚Šæ›¿ãˆï¼ˆ.envï¼‰
```bash
# Civitai APIè¨­å®š
CIVITAI_API_KEY=your_api_key_here

# Civitai ãƒ­ã‚°ã‚¤ãƒ³èªè¨¼ï¼ˆåˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«å–å¾—ç”¨ï¼‰
CIVITAI_USERNAME=your_username_here
CIVITAI_PASSWORD=your_password_here

# å‹•ä½œãƒ¢ãƒ¼ãƒ‰è¨­å®š
# DOWNLOAD_ENABLED: true/falseï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: falseï¼‰
# falseã®å ´åˆã€URLåé›†ã®ã¿å®Ÿè¡Œ
DOWNLOAD_ENABLED=false

# ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
SCRAPING_ENABLED=false  # true/falseï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: falseï¼‰
SESSION_CACHE_DIR=./cache/sessions

# å‡ºåŠ›è¨­å®š
OUTPUT_FORMAT=csv  # text, csv, json ã‹ã‚‰é¸æŠ
OUTPUT_DIR=./outputs/urls

# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰è¨­å®šï¼ˆDOWNLOAD_ENABLED=true ã®å ´åˆã®ã¿æœ‰åŠ¹ï¼‰
DOWNLOAD_DIR=./models
MAX_CONCURRENT_DOWNLOADS=1
DOWNLOAD_TIMEOUT=300  # ç§’
```

#### URLCollectorå®Ÿè£…
```python
import csv
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict, NamedTuple

class URLInfo(NamedTuple):
    """URLæƒ…å ±ã‚’æ ¼ç´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    model_id: int
    version_id: int
    model_name: str
    model_type: str
    download_url: str
    file_size: int
    tags: List[str]
    creator: str
    civitai_url: str

class URLCollector:
    """ãƒ¢ãƒ‡ãƒ«URLã®åé›†ã¨å‡ºåŠ›ã‚’ç®¡ç†"""
    
    def __init__(self, output_dir: Path = Path("outputs/urls")):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def collect_model_urls(self, models: List[dict]) -> List[URLInfo]:
        """ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰URLæƒ…å ±ã‚’åé›†"""
        urls = []
        
        for model in models:
            if not model.get('modelVersions'):
                continue
                
            version = model['modelVersions'][0]
            url_info = URLInfo(
                model_id=model['id'],
                version_id=version['id'],
                model_name=model['name'],
                model_type=model['type'],
                download_url=f"https://civitai.com/api/download/models/{version['id']}",
                file_size=version.get('files', [{}])[0].get('sizeKB', 0) * 1024,
                tags=model.get('tags', []),
                creator=model['creator']['username'],
                civitai_url=f"https://civitai.com/models/{model['id']}"
            )
            urls.append(url_info)
        
        return urls
    
    def export_to_text(self, urls: List[URLInfo], filename: str = None) -> Path:
        """URLãƒªã‚¹ãƒˆã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
        if filename is None:
            filename = f"urls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        file_path = self.output_dir / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# Civitai Model URLs - Generated at {datetime.now()}\n")
            f.write(f"# Total models: {len(urls)}\n\n")
            
            for url in urls:
                f.write(f"# {url.model_name} ({url.model_type})\n")
                f.write(f"# Tags: {', '.join(url.tags[:5])}\n")
                f.write(f"{url.download_url}\n\n")
        
        return file_path
    
    def export_to_csv(self, urls: List[URLInfo], filename: str = None) -> Path:
        """URLãƒªã‚¹ãƒˆã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
        if filename is None:
            filename = f"urls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        
        file_path = self.output_dir / filename
        
        with open(file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([
                'model_id', 'version_id', 'model_name', 'model_type',
                'download_url', 'file_size_mb', 'tags', 'creator', 'civitai_url'
            ])
            
            for url in urls:
                writer.writerow([
                    url.model_id,
                    url.version_id,
                    url.model_name,
                    url.model_type,
                    url.download_url,
                    f"{url.file_size / (1024 * 1024):.2f}",
                    ', '.join(url.tags),
                    url.creator,
                    url.civitai_url
                ])
        
        return file_path
    
    def export_to_json(self, urls: List[URLInfo], filename: str = None) -> Path:
        """URLãƒªã‚¹ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""
        if filename is None:
            filename = f"urls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        file_path = self.output_dir / filename
        
        data = {
            'generated_at': datetime.now().isoformat(),
            'total_models': len(urls),
            'models': [url._asdict() for url in urls]
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return file_path
```

#### ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®å®Ÿè£…
```python
import os
from dotenv import load_dotenv

def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
    # ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
    load_dotenv()
    
    api_key = os.getenv('CIVITAI_API_KEY')
    download_enabled = os.getenv('DOWNLOAD_ENABLED', 'false').lower() == 'true'
    output_format = os.getenv('OUTPUT_FORMAT', 'csv')
    
    # APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    client = CivitaiClient(api_key)
    filter = ModelFilter()
    
    # ãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢
    print("ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ä¸­...")
    checkpoints = client.search_checkpoints()
    loras = filter.search_loras_with_filters()
    
    all_models = checkpoints + loras
    print(f"åˆè¨ˆ {len(all_models)} å€‹ã®ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    
    # URLåé›†
    collector = URLCollector()
    urls = collector.collect_model_urls(all_models)
    
    # URLãƒªã‚¹ãƒˆã®å‡ºåŠ›
    print(f"\nURLãƒªã‚¹ãƒˆã‚’{output_format}å½¢å¼ã§å‡ºåŠ›ä¸­...")
    if output_format == 'text':
        output_file = collector.export_to_text(urls)
    elif output_format == 'csv':
        output_file = collector.export_to_csv(urls)
    elif output_format == 'json':
        output_file = collector.export_to_json(urls)
    else:
        print(f"ä¸æ˜ãªå‡ºåŠ›å½¢å¼: {output_format}")
        return
    
    print(f"URLãƒªã‚¹ãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
    
    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ï¼ˆæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
    if download_enabled:
        print("\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ã§ã™")
        downloader = ModelDownloader()
        history = DownloadHistory()
        
        for model in all_models:
            if should_download(model, history):
                print(f"\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {model['name']}")
                downloader.download_model(model)
                time.sleep(2)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
    else:
        print("\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰ã¯ç„¡åŠ¹ã§ã™ï¼ˆURLåé›†ã®ã¿ï¼‰")
        print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€.envãƒ•ã‚¡ã‚¤ãƒ«ã§ DOWNLOAD_ENABLED=true ã‚’è¨­å®šã—ã¦ãã ã•ã„")

if __name__ == "__main__":
    main()
```

### 5.7 ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè£…è©³ç´°

#### CivitaiWebScraperå®Ÿè£…ï¼ˆweb_scraper.pyï¼‰
```python
import os
import pickle
import requests
from pathlib import Path
from typing import List, Optional
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class CivitaiWebScraper:
    """CivitAIåˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«å–å¾—ç”¨ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    
    def __init__(self, username: str, password: str, cache_dir: str = "./cache/sessions"):
        self.username = username
        self.password = password
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session_file = self.cache_dir / f"session_{username}.pkl"
        
        # Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼è¨­å®š
        self.driver_options = webdriver.ChromeOptions()
        self.driver_options.add_argument("--headless")
        self.driver_options.add_argument("--no-sandbox")
        self.driver_options.add_argument("--disable-dev-shm-usage")
        self.driver = None
    
    def _init_driver(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–"""
        if not self.driver:
            self.driver = webdriver.Chrome(options=self.driver_options)
    
    def _save_session(self):
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’ä¿å­˜"""
        with open(self.session_file, 'wb') as f:
            pickle.dump(self.session.cookies, f)
    
    def _load_session(self) -> bool:
        """ä¿å­˜ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’èª­ã¿è¾¼ã¿"""
        if self.session_file.exists():
            try:
                with open(self.session_file, 'rb') as f:
                    cookies = pickle.load(f)
                    self.session.cookies.update(cookies)
                return self._validate_session()
            except Exception as e:
                print(f"ã‚»ãƒƒã‚·ãƒ§ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                return False
        return False
    
    def _validate_session(self) -> bool:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèª"""
        try:
            response = self.session.get("https://civitai.com/user/account", timeout=10)
            return response.status_code == 200 and "login" not in response.url.lower()
        except:
            return False
    
    def login(self) -> bool:
        """CivitAIã«ãƒ­ã‚°ã‚¤ãƒ³"""
        # æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ç¢ºèª
        if self._load_session():
            print("æ—¢å­˜ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨")
            return True
        
        print("æ–°è¦ãƒ­ã‚°ã‚¤ãƒ³ã‚’å®Ÿè¡Œ")
        self._init_driver()
        
        try:
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
            self.driver.get("https://civitai.com/login")
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãƒ»ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
            username_field = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.NAME, "email"))
            )
            password_field = self.driver.find_element(By.NAME, "password")
            
            username_field.send_keys(self.username)
            password_field.send_keys(self.password)
            
            # ãƒ­ã‚°ã‚¤ãƒ³ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯
            login_button = self.driver.find_element(By.XPATH, "//button[@type='submit']")
            login_button.click()
            
            # ãƒ­ã‚°ã‚¤ãƒ³å®Œäº†ã‚’å¾…æ©Ÿ
            WebDriverWait(self.driver, 10).until(
                lambda driver: "login" not in driver.current_url.lower()
            )
            
            # Seleniumã®Cookieã‚’requests.Sessionã«ç§»è¡Œ
            for cookie in self.driver.get_cookies():
                self.session.cookies.set(cookie['name'], cookie['value'], domain=cookie['domain'])
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¿å­˜
            self._save_session()
            
            print("ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"ãƒ­ã‚°ã‚¤ãƒ³ã‚¨ãƒ©ãƒ¼: {e}")
            return False
        finally:
            if self.driver:
                self.driver.quit()
                self.driver = None
    
    def get_user_models(self, target_username: str, max_pages: int = 10) -> List[str]:
        """æŒ‡å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¨ãƒ¢ãƒ‡ãƒ«URLã‚’å–å¾—"""
        if not self.login():
            raise Exception("ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸ")
        
        model_urls = []
        page = 1
        
        while page <= max_pages:
            url = f"https://civitai.com/user/{target_username}/models?page={page}"
            
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # ãƒ¢ãƒ‡ãƒ«ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                model_links = soup.find_all('a', href=lambda href: href and '/models/' in href)
                page_models = []
                
                for link in model_links:
                    href = link.get('href')
                    if href and href.startswith('/models/') and href not in model_urls:
                        full_url = f"https://civitai.com{href}"
                        model_urls.append(full_url)
                        page_models.append(full_url)
                
                print(f"ãƒšãƒ¼ã‚¸ {page}: {len(page_models)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹")
                
                # æ¬¡ãƒšãƒ¼ã‚¸ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                next_button = soup.find('a', text=lambda text: text and 'next' in text.lower())
                if not next_button or not page_models:
                    break
                
                page += 1
                time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                
            except Exception as e:
                print(f"ãƒšãƒ¼ã‚¸ {page} ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼: {e}")
                break
        
        print(f"åˆè¨ˆ {len(model_urls)} å€‹ã®ãƒ¢ãƒ‡ãƒ«URLã‚’ç™ºè¦‹")
        return model_urls
    
    def get_restricted_model_urls(self, target_username: str) -> List[str]:
        """åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«ã®URLã‚’å–å¾—ï¼ˆãƒ­ã‚°ã‚¤ãƒ³èªè¨¼ã§ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ï¼‰"""
        return self.get_user_models(target_username)
    
    def extract_model_id_from_url(self, url: str) -> Optional[int]:
        """URLã‹ã‚‰ãƒ¢ãƒ‡ãƒ«IDã‚’æŠ½å‡º"""
        import re
        match = re.search(r'/models/(\d+)', url)
        return int(match.group(1)) if match else None
    
    def maintain_session(self) -> bool:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¿æŒç¢ºèª"""
        return self._validate_session()
    
    def __del__(self):
        """ãƒ‡ã‚¹ãƒˆãƒ©ã‚¯ã‚¿"""
        if self.driver:
            self.driver.quit()

# ä½¿ç”¨ä¾‹
def collect_restricted_models(username: str, target_user: str, password: str):
    """åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«ã®ä¸€æ‹¬å–å¾—"""
    scraper = CivitaiWebScraper(username, password)
    
    try:
        # ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦URLã‚’å–å¾—
        model_urls = scraper.get_restricted_model_urls(target_user)
        
        # URLã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_file = f"restricted_models_{target_user}.txt"
        with open(output_file, 'w') as f:
            for url in model_urls:
                f.write(f"{url}\n")
        
        print(f"åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«URLã‚’ä¿å­˜: {output_file}")
        
        # ä¸€æ‹¬åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§å‡¦ç†
        return model_urls
        
    except Exception as e:
        print(f"åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        return []

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã®å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    username = os.getenv("CIVITAI_USERNAME")
    password = os.getenv("CIVITAI_PASSWORD")
    target_user = "DanMogren"
    
    if username and password:
        urls = collect_restricted_models(username, target_user, password)
        print(f"{len(urls)}å€‹ã®åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¦‹")
    else:
        print("èªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
```

#### ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±åˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
```python
def enhanced_user_collection_with_scraping(target_username: str):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨APIã‚’çµ„ã¿åˆã‚ã›ãŸåŒ…æ‹¬çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¢ãƒ‡ãƒ«åé›†"""
    
    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰èªè¨¼æƒ…å ±ã‚’å–å¾—
    api_key = os.getenv("CIVITAI_API_KEY")
    civitai_username = os.getenv("CIVITAI_USERNAME")
    civitai_password = os.getenv("CIVITAI_PASSWORD")
    scraping_enabled = os.getenv("SCRAPING_ENABLED", "false").lower() == "true"
    
    all_model_infos = []
    
    # 1. é€šå¸¸ã®APIæ¤œç´¢ï¼ˆå…¬é–‹ãƒ¢ãƒ‡ãƒ«ï¼‰
    print("ğŸ” APIçµŒç”±ã§å…¬é–‹ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ä¸­...")
    client = CivitaiClient(api_key)
    collector = EnhancedURLCollector(api_key)
    
    try:
        api_models = client.search_models_with_cursor(username=target_username, max_pages=5)
        if api_models:
            api_model_infos = collector.collect_enhanced_model_info(api_models)
            all_model_infos.extend(api_model_infos)
            print(f"âœ… APIçµŒç”±: {len(api_model_infos)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—")
    except Exception as e:
        print(f"âŒ APIå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 2. ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆåˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«ï¼‰
    if scraping_enabled and civitai_username and civitai_password:
        print("\nğŸŒ ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ä¸­...")
        
        try:
            scraper = CivitaiWebScraper(civitai_username, civitai_password)
            restricted_urls = scraper.get_restricted_model_urls(target_username)
            
            # æ—¢ã«å–å¾—æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«IDã‚’é™¤å¤–
            existing_ids = {info.model_id for info in all_model_infos}
            new_urls = []
            
            for url in restricted_urls:
                model_id = scraper.extract_model_id_from_url(url)
                if model_id and model_id not in existing_ids:
                    new_urls.append(url)
            
            print(f"ğŸ” æ–°è¦åˆ¶é™ä»˜ããƒ¢ãƒ‡ãƒ«: {len(new_urls)}å€‹ã®URLã‚’ç™ºè¦‹")
            
            # å€‹åˆ¥APIå–å¾—ã§è©³ç´°æƒ…å ±ã‚’åé›†
            for url in new_urls:
                try:
                    model_data = client.get_model_from_url(url)
                    model_info = collector.collect_enhanced_model_info([model_data])
                    all_model_infos.extend(model_info)
                    time.sleep(1)  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™
                except Exception as e:
                    print(f"âŒ å€‹åˆ¥å–å¾—ã‚¨ãƒ©ãƒ¼ ({url}): {e}")
            
            print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµŒç”±: {len(new_urls)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ å–å¾—")
            
        except Exception as e:
            print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
    
    # 3. çµæœã®çµ±åˆã¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    if all_model_infos:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = f"{target_username}_comprehensive_{timestamp}"
        
        exported_files = collector.export_all_formats(all_model_infos, base_filename)
        
        print(f"\nğŸ“ åŒ…æ‹¬çš„åé›†å®Œäº†!")
        print(f"ğŸ“Š ç·ãƒ¢ãƒ‡ãƒ«æ•°: {len(all_model_infos)}")
        print(f"ğŸ“„ CSV: {exported_files['csv']}")
        print(f"ğŸ“‹ JSON: {exported_files['json']}")
        print(f"ğŸŒ HTML: {exported_files['html']}")
        
        return all_model_infos
    else:
        print("âŒ ãƒ¢ãƒ‡ãƒ«ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return []
```

## 6. ãƒªã‚¹ã‚¯ç®¡ç†

### 6.1 æŠ€è¡“çš„ãƒªã‚¹ã‚¯
| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | å¯¾ç­– |
|--------|--------|------|
| APIä»•æ§˜å¤‰æ›´ | é«˜ | ãƒãƒ¼ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°å¯¾å¿œã€ã‚¨ãƒ©ãƒ¼é€šçŸ¥ |
| ãƒ¬ãƒ¼ãƒˆåˆ¶é™ | ä¸­ | é©å¿œçš„ãªå¾…æ©Ÿæ™‚é–“ã€ãƒªãƒˆãƒ©ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ |
| å¤§å®¹é‡ãƒ•ã‚¡ã‚¤ãƒ« | ä¸­ | ãƒãƒ£ãƒ³ã‚¯è»¢é€ã€å†é–‹æ©Ÿèƒ½ |

### 6.2 é‹ç”¨ãƒªã‚¹ã‚¯
| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | å¯¾ç­– |
|--------|--------|------|
| ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ | é«˜ | äº‹å‰ãƒã‚§ãƒƒã‚¯ã€è­¦å‘Šè¡¨ç¤º |
| ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ–­çµ¶ | ä¸­ | è‡ªå‹•å†è©¦è¡Œã€çŠ¶æ…‹ä¿å­˜ |

## 7. æˆæœç‰©

### 7.1 ãƒ—ãƒ­ã‚°ãƒ©ãƒ 
- Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼ˆpip installableï¼‰
- ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPyInstallerï¼‰

### 7.2 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰ï¼ˆæ—¥æœ¬èªï¼‰
- ä½¿ç”¨æ–¹æ³•ãƒãƒ‹ãƒ¥ã‚¢ãƒ«
- APIã‚­ãƒ¼å–å¾—æ‰‹é †æ›¸
- ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰

### 7.3 ã‚µãƒ³ãƒ—ãƒ«
- åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹
- ãƒãƒƒãƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ä¾‹

## 8. ä»Šå¾Œã®æ‹¡å¼µè¨ˆç”»

### Version 2.0
- Web UI ã®è¿½åŠ 
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œæ©Ÿèƒ½
- ãƒ¢ãƒ‡ãƒ«æ›´æ–°é€šçŸ¥
- è¤‡æ•°APIã‚­ãƒ¼å¯¾å¿œ

### Version 3.0
- ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸é€£æº
- ãƒ¢ãƒ‡ãƒ«ç®¡ç†ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
- è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
- ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ 